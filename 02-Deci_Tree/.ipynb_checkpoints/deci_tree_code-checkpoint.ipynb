{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Complexity & Overfitting Trade-off in Decision Trees on Binary Classifications**\n",
    "\n",
    "### **Author: Jiaqi Zhang**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "In this code, I will be implementing Decision Trees to solve\n",
    "binary classification problems. I will use this as a classifier to predict the result of chess\n",
    "matches and classify emails as spam. I will explore the relationship of model complexity and overfitting.\n",
    "\n",
    "### **Data**\n",
    "\n",
    "#### Spambase Dataset\n",
    "\n",
    "I will be testing your Decision Trees on a real world dataset, the\n",
    "Spambase dataset. The goal is to train a model that can classify whether\n",
    "an email is spam or not. The dataset features attributes such as the\n",
    "frequency of certain words and the amount of capital letters in a given\n",
    "message. One can find more details on the dataset\n",
    "[**here**](https://archive.ics.uci.edu/ml/datasets/spambase). I will\n",
    "only be using a subset of the full dataset.\n",
    "\n",
    "#### Chess Dataset\n",
    "\n",
    "Each row of the `chess.csv` dataset contains 36 features, which\n",
    "represent the current state of the chess board. Given this\n",
    "representation, the task is to use the Decision Trees to classify\n",
    "whether or not it is possible for white to win the game. For more\n",
    "information on the dataset, see\n",
    "[**here**](https://archive.ics.uci.edu/ml/datasets/Chess+(King-Rook+vs.+King-Pawn)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Algorithm Overview and Math**\n",
    "\n",
    "### **Decision Trees**\n",
    "\n",
    "### Part I: Generic Decision Trees in Python\n",
    "\n",
    "In this part, you will be implementing a generic decision tree for\n",
    "binary classification given binary features. Your decision tree will\n",
    "take training data\n",
    "$S = ((\\mathbf{x}_1, y_1) \\dots (\\mathbf{x}_m, y_m))$---where\n",
    "$\\mathbf{x}_{i} \\in \\{0, 1\\}^{d}$ represents the binary feature vectors\n",
    "and $y \\in \\{0, 1\\}$ are the class labels---and attempt to find a tree\n",
    "that minimizes training error. Recall that the training error for a\n",
    "hypothesis $h$ is defined as the *average* $0\\!-\\!1$ loss\n",
    "\n",
    "$$L_{S}(h) = \\frac{1}{m} \\sum\\limits_{(\\mathbf{x},y) \\in S} (y \\neq h(\\mathbf{x})).$$\n",
    "\n",
    "The primary methods of the `DecisionTree` class are as follows:\n",
    "\n",
    "-   **Functionality:**\n",
    "\n",
    "    -   `DecisionTree(data, validation_data=None, gain_function=node_score_entropy, max_depth=40)`\n",
    "        creates a `DecisionTree` that greedily minimizes training error\n",
    "        on the given dataset. The depth of the tree should not be\n",
    "        greater than `max_depth`. If $\\texttt{validation\\_data}$ is\n",
    "        passed as an argument, the validation data should be used to\n",
    "        prune the tree after it has been constructed.\n",
    "\n",
    "    -   `predict(features)` predicts a label $y \\in \\{0, 1\\}$ given\n",
    "        $\\text{features} \\in \\{0,1\\}^{d}$. Note that in our\n",
    "        implementation features are represented as Python `bool` types\n",
    "        (`True`, `False`) and class labels are Python `int`s (`0, 1`).\n",
    "\n",
    "    -   `accuracy(data)` computes accuracy, defined as\n",
    "        $1 - \\texttt{loss(self, data)}$.\n",
    "\n",
    "    -   `loss(data)` computes the training error, or the *average* loss,\n",
    "        $L_{\\texttt{data}}(h)$.\n",
    "\n",
    "-   **Helper functions:** This is where most of the algorithmic work\n",
    "    will take place. Each helper function begins with an underscore:\n",
    "    `_predict_recurs, _prune_recurs, _is_terminal, _split_recurs, _calc_gain`.\n",
    "     **Note that when splitting on the $i$-th feature, the left child will have data\n",
    "    points with $i$-th feature 0 and the right child will have data\n",
    "    points with $i$-th feature 1.**\n",
    "\n",
    "-   **Debugging:** The Given functions to help one visualize\n",
    "    decision tree for sanity check and debugging purposes.\n",
    "\n",
    "    -   `print_tree()` prints the tree to the command line. We have\n",
    "        provided a working implementation, which you are free to\n",
    "        improve. The current tree visualization works best for very\n",
    "        shallow trees.\n",
    "\n",
    "    -   `loss_plot_vec(data)` returns a vector of loss values where the\n",
    "        $i$-th element corresponds to the loss of the tree with $i$\n",
    "        nodes. The result can be plotted with `matplotlib.pyplot` to\n",
    "        visualize the loss as your tree expands.\n",
    "\n",
    "    To use these debugging functions, you must store the maximum gain at\n",
    "    each node and the number of data points that have made it to that\n",
    "    node when training using \\_set_info().\n",
    "\n",
    "\n",
    "### Part II: Measures of Gain\n",
    "\n",
    "There are multiple measures of gain that an\n",
    "algorithm can use when determining on which feature to split the current\n",
    "node. In this code, I will be implementing and comparing the\n",
    "results of three measures of gain: decrease in training error,\n",
    "information gain (entropy) and Gini index.These three measures of gain is defined as followed:\n",
    "\n",
    "![three gain](./entrop.png)\n",
    "\n",
    "\n",
    "\\\n",
    "The `DecisionTree` class takes an optional `gain_function` parameter.\n",
    "This function will be one of the three functions: `node_score_error`, `node_score_entropy` and\n",
    "`node_score_gini`. All of these gain functions should return a float.\n",
    "\n",
    "### Part III: Chess Predictions & Spam Classification\n",
    "\n",
    "-   For each dataset (`chess.csv`, `spam.csv`)\n",
    "\n",
    "    -   For each gain function (Training error, Entropy, Gini)\n",
    "\n",
    "        -   Print training loss without pruning\n",
    "\n",
    "        -   Print test loss without pruning\n",
    "\n",
    "        -   Print training loss with pruning\n",
    "\n",
    "        -   Print test loss with pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#####################################################################################################################\n",
    "# Data Processing Section\n",
    "# Helper function for preparing data for a decision tree classifiction problem. Parsing the data such\n",
    "# that for each feature, the property can only either be True or False. Label can only be 1 or 0.\n",
    "# For the chess.csv dataset won=1, nowin=0\n",
    "# In more detail:\n",
    "# Dataset with n instances, for each instance, there are m attributes. For the i-th attribute,\n",
    "# the property should be chosen from a set with size of m_i to represent the information.\n",
    "# Input: array with size of n*(m+1), the first column is the label\n",
    "# Output: array with size of n*(m_1 + m_2 + ... + m_m + 1), the first column is 1 or 0 corresponding to label\n",
    "#####################################################################################################################\n",
    "\n",
    "def get_data(filename, class_name):\n",
    "    data = read_data(filename)\n",
    "    data = convert_to_binary_features(data, class_name)\n",
    "    return np.array(split_data(data), dtype=object)\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def convert_to_binary_features(data, class_name):\n",
    "    features = []\n",
    "    for feature_index in range(0, len(data[0])-1):\n",
    "        feature_values = list(set([obs[feature_index] for obs in data]))\n",
    "        feature_values.sort()\n",
    "        if len(feature_values) > 2: features.append(feature_values[:-1])\n",
    "        else: features.append([feature_values[0]])\n",
    "    new_data = []\n",
    "    for obs in data:\n",
    "        new_obs = [1 if obs[-1] == class_name else 0] # label = 1 if label in the dataset is won\n",
    "        for feature_index in range(0, len(data[0]) - 1):\n",
    "            current_feature_value = obs[feature_index]\n",
    "            for possible_feature_value in features[feature_index]:\n",
    "                new_obs.append(current_feature_value == possible_feature_value)\n",
    "        new_data.append(new_obs)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def split_data(data, num_training=1000, num_validation=1000):\n",
    "    random.shuffle(data)\n",
    "    # casting to a numpy array\n",
    "    data = np.array(data)\n",
    "    return data[0:num_training], data[num_training:num_training + num_validation], data[num_training + num_validation:len(data)]## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "def node_score_error(prob):\n",
    "    '''\n",
    "        Calculate the node score using the train error of the subdataset and return it.\n",
    "        For a dataset with two classes, C(p) = min{p, 1-p}\n",
    "    '''\n",
    "    C_p = min(prob, 1.0 - prob)\n",
    "    return C_p\n",
    "\n",
    "def node_score_entropy(prob):\n",
    "    '''\n",
    "        Calculate the node score using the entropy of the subdataset and return it.\n",
    "        For a dataset with 2 classes, C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "        For the purposes of this calculation, assume 0*log0 = 0.\n",
    "    '''\n",
    "    if prob <= 0 or prob >= 1:\n",
    "        C_p = 0.0\n",
    "    else:\n",
    "        C_p = - prob * math.log(prob) - (1.0 - prob) * math.log(1.0 - prob)\n",
    "\n",
    "    return C_p\n",
    "\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "        Calculate the node score using the gini index of the subdataset and return it.\n",
    "        For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "    C_p = 2 * prob * (1.0 - prob)\n",
    "    return C_p\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {} # used for visualization\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        '''\n",
    "\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_entropy, max_depth=40):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        ys =  [row[0] for row in data]\n",
    "        \n",
    "        y_1_num = 0\n",
    "        for y in ys:\n",
    "            if y == 1:\n",
    "                y_1_num += 1\n",
    "        y_0_num = len(ys) - y_1_num\n",
    "\n",
    "        if y_1_num > y_0_num:\n",
    "            self.majority_class = 1\n",
    "        else:\n",
    "            self.majority_class = 0\n",
    "        \n",
    "        self.root = Node(label = self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recurs(self.root, validation_data)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        '''\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy on the given data.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss on the given data.\n",
    "        '''\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if (prediction != test_Y[i]):\n",
    "                cnt += 1.0\n",
    "        return cnt/len(data)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        Traverse the tree until leaves to get the label.\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "        if not row[split_index]:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _prune_recurs(self, node, validation_data):\n",
    "        '''\n",
    "        Prune the tree bottom up recursively. Nothing needs to be returned.\n",
    "        Do not prune if the node is a leaf.\n",
    "        Do not prune if the node is non-leaf and has at least one non-leaf child.\n",
    "        Prune if deleting the node could reduce loss on the validation data.\n",
    "        '''\n",
    "        # Checks if node is not a Leaf\n",
    "        if not node.isleaf:\n",
    "            if node.left is not None:\n",
    "                # Prune node.left\n",
    "                self._prune_recurs(node.left, validation_data)\n",
    "            if node.right is not None:\n",
    "                #  Prune node.right\n",
    "                self._prune_recurs(node.right, validation_data)\n",
    "            if (node.left.isleaf) and (node.right.isleaf):\n",
    "                # Prune node only if loss is reduced\n",
    "                loss_before = self.loss(validation_data)\n",
    "\n",
    "                saved_left = node.left\n",
    "                saved_right = node.right\n",
    "                saved_split = node.index_split_on\n",
    "                saved_isleaf = node.isleaf\n",
    "                saved_label = node.label\n",
    "\n",
    "                node.left = None\n",
    "                node.right = None\n",
    "                node.index_split_on = None\n",
    "                node.isleaf = True\n",
    "\n",
    "                loss_after = self.loss(validation_data)\n",
    "\n",
    "                if not (loss_after <= loss_before):\n",
    "                    node.left = saved_left\n",
    "                    node.right = saved_right\n",
    "                    node.index_split_on = saved_split\n",
    "                    node.isleaf = saved_isleaf\n",
    "                    node.label = saved_label\n",
    "                    \n",
    "        \n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        '''\n",
    "        Helper function to determine whether the node should stop splitting.\n",
    "        Stop the recursion if:\n",
    "            1. The dataset (as passed to parameter data) is empty.\n",
    "            2. There are no more indices to split on.\n",
    "            3. All the instances in this dataset belong to the same class\n",
    "            4. The depth of the node reaches the maximum depth.\n",
    "        Set the node label to be the majority label of the training dataset if:\n",
    "            1. The number of class 1 points is equal to the number of class 0 points.\n",
    "            2. The dataset is empty.\n",
    "        Return:\n",
    "            - A boolean, True indicating the current node should be a leaf and \n",
    "              False if the node is not a leaf.\n",
    "            - A label, indicating the label of the leaf (or the label the node would \n",
    "              be if we were to terminate at that node). If there is no data left, it\n",
    "              must return the majority class of the training set.\n",
    "        '''\n",
    "        y = [row[0] for row in data]        \n",
    "        \n",
    "        # Check Cases if the node should stop splitting\n",
    "        if len(data) == 0:\n",
    "            leaf_bool = True\n",
    "            label = self.majority_class\n",
    "            return leaf_bool, label\n",
    "\n",
    "        elif len(indices) == 0:\n",
    "            leaf_bool = True\n",
    "            y_1_num = 0\n",
    "            for a in y:\n",
    "                if a == 1:\n",
    "                    y_1_num += 1\n",
    "            y_0_num = len(y) - y_1_num\n",
    "\n",
    "            if y_1_num > y_0_num:\n",
    "                local_maj = 1\n",
    "            elif y_1_num < y_0_num:\n",
    "                local_maj = 0\n",
    "            else:\n",
    "                local_maj = self.majority_class\n",
    "                \n",
    "            label = local_maj\n",
    "            return leaf_bool, label\n",
    "\n",
    "        elif sum(y) == len(y) or sum(y) == 0:\n",
    "            leaf_bool = True\n",
    "            if sum(y) == len(y):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            return leaf_bool, label\n",
    "\n",
    "        elif node.depth == self.max_depth:\n",
    "            leaf_bool = True\n",
    "            y_1_num = 0\n",
    "            for a in y:\n",
    "                if a == 1:\n",
    "                    y_1_num += 1\n",
    "            y_0_num = len(y) - y_1_num\n",
    "\n",
    "            if y_1_num > y_0_num:\n",
    "                local_maj = 1\n",
    "            elif y_1_num < y_0_num:\n",
    "                local_maj = 0\n",
    "            else:\n",
    "                local_maj = self.majority_class\n",
    "            label = local_maj\n",
    "            return leaf_bool, label\n",
    "\n",
    "        else:\n",
    "            leaf_bool = False\n",
    "            y_1_num = 0\n",
    "            for a in y:\n",
    "                if a == 1:\n",
    "                    y_1_num += 1\n",
    "            y_0_num = len(y) - y_1_num\n",
    "\n",
    "            if y_1_num > y_0_num:\n",
    "                local_maj = 1\n",
    "            elif y_1_num < y_0_num:\n",
    "                local_maj = 0\n",
    "            else:\n",
    "                local_maj = self.majority_class\n",
    "            label = local_maj\n",
    "            return leaf_bool, label\n",
    "        \n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        '''\n",
    "        Recursively split the node based on the rows and indices given.\n",
    "        Nothing needs to be returned.\n",
    "\n",
    "        First use _is_terminal() to check if the node needs to be split.\n",
    "        If so, select the column that has the maximum infomation gain to split on.\n",
    "        Store the label predicted for this node, the split column, and use _set_info()\n",
    "        to keep track of the gain and the number of datapoints at the split.\n",
    "        Then, split the data based on its value in the selected column.\n",
    "        The data should be recursively passed to the children.\n",
    "        '''\n",
    "        # Check if node needs to be split\n",
    "        node.isleaf = self._is_terminal(node, data, indices)[0]\n",
    "\n",
    "        if node.isleaf:\n",
    "            node.label = self._is_terminal(node, data, indices)[1]\n",
    "            return\n",
    "        \n",
    "        if not node.isleaf:\n",
    "            max_gain = 0.0\n",
    "            best_idx = 1\n",
    "            for idx in indices:\n",
    "                if self._calc_gain(data, idx, self.gain_function) >= max_gain:\n",
    "                    max_gain = self._calc_gain(data, idx, self.gain_function)\n",
    "                    best_idx = idx\n",
    "            node.label = self._is_terminal(node, data, indices)[1]\n",
    "            node.index_split_on = best_idx\n",
    "            node._set_info(max_gain, len(data))\n",
    "\n",
    "            left_data = [row for row in data if row[best_idx] == 0]\n",
    "            right_data = [row for row in data if row[best_idx] == 1]\n",
    "            \n",
    "            next_indices = [i for i in indices if i != best_idx]\n",
    "            \n",
    "            node.left = Node(depth=node.depth+1)\n",
    "            node.right = Node(depth=node.depth+1)\n",
    "            self._split_recurs(node.left, left_data, next_indices)\n",
    "            self._split_recurs(node.right, right_data, next_indices)\n",
    "        \n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function):\n",
    "        '''\n",
    "        Calculate the gain of the proposed splitting and return it.\n",
    "        Gain = C(P[y=1]) - P[x_i=True] * C(P[y=1|x_i=True]) - P[x_i=False] * C(P[y=0|x_i=False])\n",
    "        Here the C(p) is the gain_function. For example, if C(p) = min(p, 1-p), this would be\n",
    "        considering training error gain. Other alternatives are entropy and gini functions.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "\n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            P_y_1 = len([row[0] for row in data if row[0] == 1]) / len(y)\n",
    "            P_x_i_true = len([row[split_index] for row in data if row[split_index] == 1]) / len(xi)\n",
    "            P_x_i_fals = 1.0 - P_x_i_true\n",
    "            S_x_1 = [row for row in data if row[split_index] == 1]  \n",
    "            S_x_0 = [row for row in data if row[split_index] == 0]\n",
    "            if len(S_x_1) != 0:\n",
    "                P_y_1_x_1 = len([row for row in S_x_1 if row[0] == 1]) / len(S_x_1)\n",
    "            else:\n",
    "                P_y_1_x_1 = 0.0\n",
    "            if len(S_x_0) != 0:\n",
    "                P_y_1_x_0 = len([row for row in S_x_0 if row[0] == 1]) / len(S_x_0)\n",
    "            else:\n",
    "                P_y_1_x_0 = 0.0\n",
    "            \n",
    "            gain = gain_function(P_y_1) - (P_x_i_true * gain_function(P_y_1_x_1) + P_x_i_fals * gain_function(P_y_1_x_0))\n",
    "          \n",
    "        else:\n",
    "            gain = 0\n",
    "        return gain\n",
    "    \n",
    "\n",
    "    def print_tree(self):\n",
    "        '''\n",
    "        Helper function for tree_visualization.\n",
    "        Only effective with very shallow trees.\n",
    "        '''\n",
    "        print('---START PRINT TREE---')\n",
    "        def print_subtree(node, indent=''):\n",
    "            if node is None:\n",
    "                return str(\"None\")\n",
    "            if node.isleaf:\n",
    "                return str(node.label)\n",
    "            else:\n",
    "                decision = 'split attribute = {:d}; gain = {:f}; number of samples = {:d}'.format(node.index_split_on, node.info['gain'], node.info['num_samples'])\n",
    "            left = indent + '0 -> '+ print_subtree(node.left, indent + '\\t\\t')\n",
    "            right = indent + '1 -> '+ print_subtree(node.right, indent + '\\t\\t')\n",
    "            return (decision + '\\n' + left + '\\n' + right)\n",
    "\n",
    "        print(print_subtree(self.root))\n",
    "        print('----END PRINT TREE---')\n",
    "\n",
    "\n",
    "    def loss_plot_vec(self, data):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        '''\n",
    "        self._loss_plot_recurs(self.root, data, 0)\n",
    "        loss_vec = []\n",
    "        q = [self.root]\n",
    "        num_correct = 0\n",
    "        while len(q) > 0:\n",
    "            node = q.pop(0)\n",
    "            num_correct = num_correct + node.info['curr_num_correct']\n",
    "            loss_vec.append(num_correct)\n",
    "            if node.left != None:\n",
    "                q.append(node.left)\n",
    "            if node.right != None:\n",
    "                q.append(node.right)\n",
    "\n",
    "        return 1 - np.array(loss_vec)/len(data)\n",
    "\n",
    "\n",
    "    def _loss_plot_recurs(self, node, rows, prev_num_correct):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        '''\n",
    "        labels = [row[0] for row in rows]\n",
    "        curr_num_correct = labels.count(node.label) - prev_num_correct\n",
    "        node.info['curr_num_correct'] = curr_num_correct\n",
    "\n",
    "        if not node.isleaf:\n",
    "            left_data, right_data = [], []\n",
    "            left_num_correct, right_num_correct = 0, 0\n",
    "            for row in rows:\n",
    "                if not row[node.index_split_on]:\n",
    "                    left_data.append(row)\n",
    "                else:\n",
    "                    right_data.append(row)\n",
    "\n",
    "            left_labels = [row[0] for row in left_data]\n",
    "            left_num_correct = left_labels.count(node.label)\n",
    "            right_labels = [row[0] for row in right_data]\n",
    "            right_num_correct = right_labels.count(node.label)\n",
    "\n",
    "            if node.left != None:\n",
    "                self._loss_plot_recurs(node.left, left_data, left_num_correct)\n",
    "            if node.right != None:\n",
    "                self._loss_plot_recurs(node.right, right_data, right_num_correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---START PRINT TREE---\n",
      "split attribute = 3; gain = 0.318257; number of samples = 6\n",
      "0 -> 0\n",
      "1 -> split attribute = 2; gain = 0.174416; number of samples = 3\n",
      "\t\t0 -> 1\n",
      "\t\t1 -> split attribute = 1; gain = 0.693147; number of samples = 2\n",
      "\t\t\t\t0 -> 1\n",
      "\t\t\t\t1 -> 0\n",
      "----END PRINT TREE---\n",
      "training loss not pruned: 0.0\n",
      "validation loss not pruned: 0.5 \n",
      "\n",
      "---START PRINT TREE---\n",
      "split attribute = 3; gain = 0.318257; number of samples = 6\n",
      "0 -> 0\n",
      "1 -> 1\n",
      "----END PRINT TREE---\n",
      "training loss pruned: 0.16666666666666666\n",
      "validation loss pruned: 0.0\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jiaqi Zhang\n",
      "2026-01-22\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Tests for node_score_error\n",
    "assert node_score_error(.3) == .3\n",
    "assert node_score_error(.6) == .4\n",
    "\n",
    "# Tests for node_score_entropy\n",
    "assert node_score_entropy(.5) == pytest.approx(.69, .01)\n",
    "assert node_score_entropy(0) == node_score_entropy(1) == 0\n",
    "assert node_score_entropy(.7) == pytest.approx(.61,.01)\n",
    "\n",
    "# Tests for node_score_gini\n",
    "assert node_score_gini(1) == node_score_gini(0) == 0\n",
    "assert node_score_gini(.4) == .48\n",
    "\n",
    "# Creates Test Model and Dummy Data\n",
    "x = np.array([[0,1,0,0],[1,0,1,1],[1,1,0,1],[0,0,1,0],[0,1,1,1],[0,0,0,0]])\n",
    "test_model = DecisionTree(x)\n",
    "\n",
    "# Test for majority_class\n",
    "assert test_model.majority_class == 0\n",
    "# test checking that the first column is used to calculate the majority class\n",
    "x_majority_class_test = np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[0,0,1,0],[0,1,1,1],[0,0,0,0]])\n",
    "majority_class_test_model = DecisionTree(x_majority_class_test)\n",
    "assert majority_class_test_model.majority_class == 0\n",
    "\n",
    "# Tests for _is_terminal\n",
    "node1 = Node(left=None, right=None, depth=0, index_split_on=3, isleaf=False, label=0)\n",
    "x_filtered_node2 = np.array([row for row in x if row[3] == 1])\n",
    "node2 = Node(left=None, right=None, depth=1, index_split_on=1, isleaf=False, label=1)\n",
    "x_filtered_node3 = np.array([row for row in x_filtered_node2 if row[1] == 1])\n",
    "node3 = Node(left=None, right=None, depth=2, index_split_on=2, isleaf=False, label=0)\n",
    "x_filtered_node4 = np.array([row for row in x_filtered_node3 if row[2] == 1])\n",
    "node4 = Node(left=None, right=None, depth=3, index_split_on=None, isleaf=True, label=0)\n",
    "\n",
    "\n",
    "assert test_model._is_terminal(node=node1, data=x, indices=[1, 2, 3]) == (False, 0)\n",
    "assert test_model._is_terminal(node=node2, data=x_filtered_node2, indices=[1, 2]) == (False, 1)\n",
    "assert test_model._is_terminal(node=node3, data=x_filtered_node3, indices=[2]) == (False, 0)\n",
    "assert test_model._is_terminal(node=node4, data=x_filtered_node4, indices=[]) == (True, 0)\n",
    "\n",
    "\n",
    "# Tests _calc_gain\n",
    "# Testing gain for index 3\n",
    "assert test_model._calc_gain(x, 3, node_score_error) == pytest.approx(0.166, .01)\n",
    "assert test_model._calc_gain(x, 3, node_score_entropy) == pytest.approx(0.318, .01)\n",
    "assert test_model._calc_gain(x, 3, node_score_gini) == pytest.approx(0.222, .01)\n",
    "\n",
    "# Testing gain for index 1\n",
    "assert test_model._calc_gain(x_filtered_node2, 1, node_score_error) == pytest.approx(5.551115123125783e-17, abs=1e-18)\n",
    "assert test_model._calc_gain(x_filtered_node2, 1, node_score_entropy) == pytest.approx(0.174, .01)\n",
    "assert test_model._calc_gain(x_filtered_node2, 1, node_score_gini) == pytest.approx(0.111, .01)\n",
    "\n",
    "# Testing gain for index 2\n",
    "assert test_model._calc_gain(x_filtered_node3, 2, node_score_error) == pytest.approx(0.5, .01)\n",
    "assert test_model._calc_gain(x_filtered_node3, 2, node_score_entropy) == pytest.approx(0.693, .01)\n",
    "assert test_model._calc_gain(x_filtered_node3, 2, node_score_gini) == pytest.approx(0.5, .01)\n",
    "\n",
    "# testing gain for case when - P[x_i=False] * C(P[y=0|x_i=False] is nonzero\n",
    "new_dummy_data = [[0,1,0,],[1,0,1],[1,1,0],[0,0,1],[0,1,1],[0,0,0]]\n",
    "# testing gain for index 1\n",
    "assert test_model._calc_gain(new_dummy_data, 1, node_score_error) == pytest.approx(0.0, .01)\n",
    "assert test_model._calc_gain(new_dummy_data, 1, node_score_entropy) == pytest.approx(0.0, .01)\n",
    "assert test_model._calc_gain(new_dummy_data, 1, node_score_gini) == pytest.approx(0.0, .01)\n",
    "# testing gain for index 2\n",
    "assert test_model._calc_gain(new_dummy_data, 1, node_score_error) == pytest.approx(0.0, .01)\n",
    "assert test_model._calc_gain(new_dummy_data, 2, node_score_entropy) == pytest.approx(0.0, .01)\n",
    "assert test_model._calc_gain(new_dummy_data, 2, node_score_gini) == pytest.approx(0.0, .01)\n",
    "\n",
    "\n",
    "# Check Tree is created Properly, Compare with text below\n",
    "test_model.print_tree()\n",
    "\n",
    "# Tests _prune_recurs\n",
    "# Pruned tree should be smaller\n",
    "# with higher training loss and lower validation loss\n",
    "x_val = np.array([[1,1,1,1],[1,0,0,1]])\n",
    "\n",
    "print('training loss not pruned:', test_model.loss(x))\n",
    "print('validation loss not pruned:', test_model.loss(x_val), '\\n')\n",
    "\n",
    "test_model_pruned = DecisionTree(x,validation_data=x_val)\n",
    "test_model_pruned.print_tree()\n",
    "print('training loss pruned:', test_model_pruned.loss(x))\n",
    "print('validation loss pruned:', test_model_pruned.loss(x_val))\n",
    "\n",
    "# checking that the pruned tree root is not a leaf and has the correct label\n",
    "assert test_model_pruned.root.label == 0\n",
    "assert not test_model_pruned.root.isleaf\n",
    "# the pruned tree should have leaves with correct labels (tests to confirm the visual printout of the tree)\n",
    "assert test_model_pruned.root.right.label == 1\n",
    "assert test_model_pruned.root.left.label == 0\n",
    "assert test_model_pruned.root.right.isleaf\n",
    "assert test_model_pruned.root.left.isleaf\n",
    "\n",
    "from datetime import date\n",
    "## Print name with the date \n",
    "print()\n",
    "print('----'*25)\n",
    "print('Jiaqi Zhang')\n",
    "print(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decision Trees should look similar to below (the second one is the pruned tree)\n",
    "\n",
    "---START PRINT TREE---\n",
    "split attribute = 3; gain = 0.318257; number of samples = 6\n",
    "0 -> False\n",
    "1 -> split attribute = 1; gain = 0.174416; number of samples = 3\n",
    "\t\t0 -> True\n",
    "\t\t1 -> split attribute = 2; gain = 0.693147; number of samples = 2\n",
    "\t\t\t\t0 -> True\n",
    "\t\t\t\t1 -> False\n",
    "----END PRINT TREE---\n",
    "\n",
    "\n",
    "---START PRINT TREE---\n",
    "split attribute = 3; gain = 0.318257; number of samples = 6\n",
    "0 -> False\n",
    "1 -> True\n",
    "----END PRINT TREE---\n",
    "training loss pruned: 0.16666666666666666\n",
    "validation loss pruned: 0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---START PRINT TREE---\n",
      "split attribute = 7; gain = 0.074266; number of samples = 50\n",
      "0 -> 1\n",
      "1 -> split attribute = 2; gain = 0.028476; number of samples = 46\n",
      "\t\t0 -> split attribute = 3; gain = 0.323642; number of samples = 8\n",
      "\t\t\t\t0 -> split attribute = 5; gain = 0.219512; number of samples = 6\n",
      "\t\t\t\t\t\t0 -> 1\n",
      "\t\t\t\t\t\t1 -> 0\n",
      "\t\t\t\t1 -> 0\n",
      "\t\t1 -> split attribute = 8; gain = 0.020641; number of samples = 38\n",
      "\t\t\t\t0 -> 0\n",
      "\t\t\t\t1 -> split attribute = 5; gain = 0.003781; number of samples = 36\n",
      "\t\t\t\t\t\t0 -> 0\n",
      "\t\t\t\t\t\t1 -> 0\n",
      "----END PRINT TREE---\n",
      "training loss not pruned: 0.26\n",
      "validation loss not pruned: 0.22 \n",
      "\n",
      "---START PRINT TREE---\n",
      "split attribute = 7; gain = 0.074266; number of samples = 50\n",
      "0 -> 1\n",
      "1 -> split attribute = 2; gain = 0.028476; number of samples = 46\n",
      "\t\t0 -> split attribute = 3; gain = 0.323642; number of samples = 8\n",
      "\t\t\t\t0 -> 1\n",
      "\t\t\t\t1 -> 0\n",
      "\t\t1 -> 0\n",
      "----END PRINT TREE---\n",
      "training loss pruned: 0.26\n",
      "validation loss pruned: 0.16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "toy_train = pd.read_csv(\"./data/toy_train.csv\", header=None).to_numpy()\n",
    "toy_val = pd.read_csv(\"./data/toy_val.csv\", header=None).to_numpy()\n",
    "\n",
    "test_model = DecisionTree(toy_train, max_depth=4)\n",
    "test_model.print_tree()\n",
    "print('training loss not pruned:', test_model.loss(toy_train))\n",
    "print('validation loss not pruned:', test_model.loss(toy_val), '\\n')\n",
    "\n",
    "test_model_pruned = DecisionTree(toy_train, validation_data=toy_val, max_depth=4)\n",
    "test_model_pruned.print_tree()\n",
    "print('training loss pruned:', test_model_pruned.loss(toy_train))\n",
    "print('validation loss pruned:', test_model_pruned.loss(toy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''---START PRINT TREE---\n",
    "Decision Trees should look similar to below (the second one is the pruned tree)\n",
    "\n",
    "split attribute = 7; gain = 0.074266; number of samples = 50\n",
    "0 -> 1\n",
    "1 -> split attribute = 2; gain = 0.028476; number of samples = 46\n",
    "\t\t0 -> split attribute = 3; gain = 0.323642; number of samples = 8\n",
    "\t\t\t\t0 -> split attribute = 5; gain = 0.219512; number of samples = 6\n",
    "\t\t\t\t\t\t0 -> 1\n",
    "\t\t\t\t\t\t1 -> 0\n",
    "\t\t\t\t1 -> 0\n",
    "\t\t1 -> split attribute = 8; gain = 0.020641; number of samples = 38\n",
    "\t\t\t\t0 -> 0\n",
    "\t\t\t\t1 -> split attribute = 5; gain = 0.003781; number of samples = 36\n",
    "\t\t\t\t\t\t0 -> 0\n",
    "\t\t\t\t\t\t1 -> 0\n",
    "----END PRINT TREE---\n",
    "training loss not pruned: 0.26\n",
    "validation loss not pruned: 0.22 \n",
    "\n",
    "---START PRINT TREE---\n",
    "split attribute = 7; gain = 0.074266; number of samples = 50\n",
    "0 -> 1\n",
    "1 -> split attribute = 2; gain = 0.028476; number of samples = 46\n",
    "\t\t0 -> split attribute = 3; gain = 0.323642; number of samples = 8\n",
    "\t\t\t\t0 -> 1\n",
    "\t\t\t\t1 -> 0\n",
    "\t\t1 -> 0\n",
    "----END PRINT TREE---\n",
    "training loss pruned: 0.26\n",
    "validation loss pruned: 0.16'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess Dataset\n",
      "Average training loss (not-pruned) for the entropy measure of gain: 0.0\n",
      "Average test loss (not-pruned) for the entropy measure of gain: 0.019230769230769232\n",
      "Average training loss (pruned) for the entropy measure of gain: 0.007\n",
      "Average test loss (pruned) for the entropy measure of gain: 0.016722408026755852\n",
      "\n",
      "Average training loss (not-pruned) for the training error measure of gain: 0.015\n",
      "Average test loss (not-pruned) for the training error measure of gain: 0.05351170568561873\n",
      "Average training loss (pruned) for the training error measure of gain: 0.026\n",
      "Average test loss (pruned) for the training error measure of gain: 0.051839464882943144\n",
      "\n",
      "Average training loss (not-pruned) for the gini measure of gain: 0.0\n",
      "Average test loss (not-pruned) for the gini measure of gain: 0.019230769230769232\n",
      "Average training loss (pruned) for the gini measure of gain: 0.007\n",
      "Average test loss (pruned) for the gini measure of gain: 0.016722408026755852\n",
      "\n",
      "Spam Dataset\n",
      "Average training loss (not-pruned) for the entropy measure of gain: 0.014\n",
      "Average test loss (not-pruned) for the entropy measure of gain: 0.12110726643598616\n",
      "Average training loss (pruned) for the entropy measure of gain: 0.041\n",
      "Average test loss (pruned) for the entropy measure of gain: 0.11072664359861592\n",
      "\n",
      "Average training loss (not-pruned) for the training error measure of gain: 0.03\n",
      "Average test loss (not-pruned) for the training error measure of gain: 0.11995386389850057\n",
      "Average training loss (pruned) for the training error measure of gain: 0.054\n",
      "Average test loss (pruned) for the training error measure of gain: 0.11418685121107267\n",
      "\n",
      "Average training loss (not-pruned) for the gini measure of gain: 0.014\n",
      "Average test loss (not-pruned) for the gini measure of gain: 0.12572087658592848\n",
      "Average training loss (pruned) for the gini measure of gain: 0.042\n",
      "Average test loss (pruned) for the gini measure of gain: 0.12033833141099577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loss_plot(ax, title, tree, pruned_tree, train_data, test_data):\n",
    "    '''\n",
    "        Example plotting code. This plots four curves: the training and testing\n",
    "        average loss using tree and pruned tree.\n",
    "        Arguments:\n",
    "            - ax: A matplotlib Axes instance.\n",
    "            - title: A title for the graph (string)\n",
    "            - tree: An unpruned DecisionTree instance\n",
    "            - pruned_tree: A pruned DecisionTree instance\n",
    "            - train_data: Training dataset returned from get_data\n",
    "            - test_data: Test dataset returned from get_data\n",
    "    '''\n",
    "    fontsize=8\n",
    "    ax.plot(tree.loss_plot_vec(train_data), label='train non-pruned')\n",
    "    ax.plot(tree.loss_plot_vec(test_data), label='test non-pruned')\n",
    "    ax.plot(pruned_tree.loss_plot_vec(train_data), label='train pruned')\n",
    "    ax.plot(pruned_tree.loss_plot_vec(test_data), label='test pruned')\n",
    "\n",
    "\n",
    "    ax.locator_params(nbins=3)\n",
    "    ax.set_xlabel('number of nodes', fontsize=fontsize)\n",
    "    ax.set_ylabel('loss', fontsize=fontsize)\n",
    "    ax.set_title(title, fontsize=fontsize)\n",
    "    legend = ax.legend(loc='upper center', shadow=True, fontsize=fontsize-2)\n",
    "\n",
    "def explore_dataset(filename, class_name):\n",
    "    train_data, validation_data, test_data = get_data(filename, class_name)\n",
    "\n",
    "    # For each measure of gain (training error, entropy, gini):\n",
    "    #      (a) Print average training loss (not-pruned)\n",
    "    #      (b) Print average test loss (not-pruned)\n",
    "    #      (c) Print average training loss (pruned)\n",
    "    #      (d) Print average test loss (pruned)\n",
    "\n",
    "    model_1 =  DecisionTree(train_data, validation_data=None, gain_function=node_score_entropy, max_depth=40)\n",
    "    model_1_pruned = DecisionTree(train_data, validation_data=validation_data, gain_function=node_score_entropy, max_depth=40)\n",
    "\n",
    "    model_2 = DecisionTree(train_data, validation_data=None, gain_function=node_score_error, max_depth=40)\n",
    "    model_2_pruned = DecisionTree(train_data, validation_data=validation_data, gain_function=node_score_error, max_depth=40)\n",
    "\n",
    "    model_3 = DecisionTree(train_data, validation_data=None, gain_function=node_score_gini, max_depth=40)\n",
    "    model_3_pruned = DecisionTree(train_data, validation_data=validation_data, gain_function=node_score_gini, max_depth=40)\n",
    "\n",
    "    if class_name == 'won':\n",
    "        print('Chess Dataset')\n",
    "    if class_name == '1':\n",
    "        print('Spam Dataset')\n",
    "    print('Average training loss (not-pruned) for the entropy measure of gain:', model_1.loss(train_data))\n",
    "    print('Average test loss (not-pruned) for the entropy measure of gain:', model_1.loss(test_data))\n",
    "    print('Average training loss (pruned) for the entropy measure of gain:', model_1_pruned.loss(train_data))\n",
    "    print('Average test loss (pruned) for the entropy measure of gain:', model_1_pruned.loss(test_data))\n",
    "    print()\n",
    "    print('Average training loss (not-pruned) for the training error measure of gain:', model_2.loss(train_data))\n",
    "    print('Average test loss (not-pruned) for the training error measure of gain:', model_2.loss(test_data))\n",
    "    print('Average training loss (pruned) for the training error measure of gain:', model_2_pruned.loss(train_data))\n",
    "    print('Average test loss (pruned) for the training error measure of gain:', model_2_pruned.loss(test_data))\n",
    "    print()\n",
    "    print('Average training loss (not-pruned) for the gini measure of gain:', model_3.loss(train_data))\n",
    "    print('Average test loss (not-pruned) for the gini measure of gain:', model_3.loss(test_data))\n",
    "    print('Average training loss (pruned) for the gini measure of gain:', model_3_pruned.loss(train_data))\n",
    "    print('Average test loss (pruned) for the gini measure of gain:', model_3_pruned.loss(test_data))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "explore_dataset('data/chess.csv', 'won')\n",
    "explore_dataset('data/spam.csv', '1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Results Report** \n",
    "\n",
    "### **Analysis on Differences in Training and Test Error of Pruned and Non-pruned Trees**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gain measure</th>\n",
       "      <th>Training loss (not-pruned)</th>\n",
       "      <th>Test loss (not-pruned)</th>\n",
       "      <th>Train loss (pruned)</th>\n",
       "      <th>Test loss (pruned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entropy</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training error</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.0518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gain measure  Training loss (not-pruned)  Test loss (not-pruned)  \\\n",
       "0         Entropy                       0.000                  0.0192   \n",
       "1  Training error                       0.015                  0.0535   \n",
       "2            Gini                       0.000                  0.0192   \n",
       "\n",
       "   Train loss (pruned)  Test loss (pruned)  \n",
       "0                0.007              0.0167  \n",
       "1                0.026              0.0518  \n",
       "2                0.007              0.0167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spam Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gain measure</th>\n",
       "      <th>Training loss (not-pruned)</th>\n",
       "      <th>Test loss (not-pruned)</th>\n",
       "      <th>Train loss (pruned)</th>\n",
       "      <th>Test loss (pruned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entropy</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.1211</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training error</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gini</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.1203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gain measure  Training loss (not-pruned)  Test loss (not-pruned)  \\\n",
       "0         Entropy                       0.014                  0.1211   \n",
       "1  Training error                       0.030                  0.1199   \n",
       "2            Gini                       0.014                  0.1257   \n",
       "\n",
       "   Train loss (pruned)  Test loss (pruned)  \n",
       "0                0.041              0.1107  \n",
       "1                0.054              0.1142  \n",
       "2                0.042              0.1203  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_chess = {\"Gain measure\": [\"Entropy\", \"Training error\", \"Gini\"],\n",
    "            \"Training loss (not-pruned)\": [0.0000, 0.0150, 0.0000],\n",
    "            \"Test loss (not-pruned)\": [0.0192, 0.0535, 0.0192],\n",
    "            \"Train loss (pruned)\": [0.0070, 0.0260, 0.0070],\n",
    "            \"Test loss (pruned)\": [0.0167, 0.0518, 0.0167]}\n",
    "\n",
    "data_spam = {\"Gain measure\": [\"Entropy\", \"Training error\", \"Gini\"],\n",
    "            \"Training loss (not-pruned)\": [0.0140, 0.0300, 0.0140],\n",
    "            \"Test loss (not-pruned)\": [0.1211, 0.1199, 0.1257],\n",
    "            \"Train loss (pruned)\": [0.0410, 0.0540, 0.0420],\n",
    "            \"Test loss (pruned)\": [0.1107, 0.1142, 0.1203]}\n",
    "\n",
    "print(\"Chess Dataset\")\n",
    "display(pd.DataFrame(data_chess))\n",
    "\n",
    "print(\"\\nSpam Dataset\")\n",
    "display(pd.DataFrame(data_spam))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For both datasets, not-pruned tree trained model gives us the lower training loss but higher test loss comparing to the pruned tree trained model. For example, in the chess dataset, the training loss of not pruned tree for entropy gain measurement is 0.000, and the test loss is 0.0192. Meanwhile, the training loss of pruned tree for entropy gain measurement is 0.007, and the test loss is 0.0167. In the spam dataset, for example, the training loss of not pruned tree for entropy gain measurement is 0.014, and the test loss is 0.1211. Meanwhile, the training loss of pruned tree for entropy gain measurement is 0.041, and the test loss is 0.1107. From the table, both entropy and gini gain measures reduce the training error most effectively than the training error as the gain measure. The pruning is effective here because the pruned trees give us the lower test loss across all gain measures and datasets, which indicates that pruning trees can suppress the overfitting and make it generalize better on unseen test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore the Training Loss As The Maximum Tree Depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, validation_data, test_data = get_data('data/spam.csv', '1')\n",
    "max_depth = list(range(1, 16))\n",
    "train_loss = []\n",
    "for depth in max_depth:\n",
    "    model = DecisionTree(train_data, validation_data=None, gain_function=node_score_entropy, max_depth=depth)\n",
    "    loss = model.loss(train_data)\n",
    "    train_loss.append(loss)\n",
    "plt.figure\n",
    "plt.plot(max_depth, train_loss, marker='o')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.xticks(max_depth)\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss VS. Maximum Tree Depth Plot of Spam Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot of the spam dataset, we can see the trend that the training loss decreases as the maximum tree depth increases. Specifically, the training loss decreases sharply as the maximum tree depth is increasing from the 1 to 4, and the training loss decreases very slow after the maximum tree depth reaches 8. This is because the more maximum tree depth is, the more complicated relationship the tree can model, therefore fitting the training data better. And after a certain maximum depth, the return of reducing the training loss will reduce. However, the larger value of the maximum tree depth can lead to the overfitting problem, so it is better to use the test data to ensure it is not overfitting the data by pruning the tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data2060)",
   "language": "python",
   "name": "data2060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
